{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAMA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Bottleneck unit testing available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 07:33:08.887645: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-23 07:33:08.887687: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-23 07:33:08.888441: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-23 07:33:08.893180: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-23 07:33:09.705731: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Analyze the sentiment of the sentence enclosed in square brackets, determine if it is happiness or fear or anger or sadness or disgust or shame or guilt or surprise, and return the answer by one word as the corresponding sentiment label 'happiness' or 'fear' or 'anger' or 'sadness' or 'disgust' or 'shame' or'guilt' or 'surprise'.\n",
    "            [{data_point[\"preprocessed_text\"]}] = {data_point[\"emotion\"]}\"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"Analyze the sentiment of the sentence enclosed in square brackets, determine if it is happiness or fear or anger or sadness or disgust or shame or guilt or surprise, and return the answer by one word as the corresponding sentiment label 'happiness' or 'fear' or 'anger' or 'sadness' or 'disgust' or 'shame' or'guilt' or 'surprise'.\n",
    "            [{data_point[\"preprocessed_text\"]}] = \"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(test))):\n",
    "        prompt = test.iloc[i][\"text\"]\n",
    "        pipe = pipeline(task=\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        max_new_tokens = 10,\n",
    "                        temperature = 0.1, \n",
    "                        return_full_text=False\n",
    "                       )\n",
    "        result = pipe(prompt)\n",
    "        generated_text = result[0]['generated_text']\n",
    "\n",
    "        emotion = generated_text.split()[0]\n",
    "        answer = ''.join([x for x in emotion if x.isalnum()])\n",
    "\n",
    "        if \"happiness\" in answer:\n",
    "            y_pred.append(\"happiness\")\n",
    "        elif \"fear\" in answer:\n",
    "            y_pred.append(\"fear\")\n",
    "        elif \"anger\" in answer:\n",
    "            y_pred.append(\"anger\")\n",
    "        elif \"sadness\" in answer:\n",
    "            y_pred.append(\"sadness\")\n",
    "        elif \"disgust\" in answer:\n",
    "            y_pred.append(\"disgust\")\n",
    "        elif \"shame\" in answer:\n",
    "            y_pred.append(\"shame\")\n",
    "        elif \"guilt\" in answer:\n",
    "            y_pred.append(\"guilt\")\n",
    "        elif \"surprise\" in answer:\n",
    "            y_pred.append(\"surprise\")\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    labels = ['happiness', 'fear', 'anger', 'sadness', 'disgust', 'shame',\n",
    "       'guilt', 'surprise']\n",
    "    mapping = {\n",
    "    'happiness': 0,\n",
    "    'fear': 1,\n",
    "    'anger': 2,\n",
    "    'sadness': 3,\n",
    "    'disgust': 4,\n",
    "    'shame': 5,\n",
    "    'guilt': 6,\n",
    "    'surprise': 7, \n",
    "    'none': 8\n",
    "}\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, 1)\n",
    "    \n",
    "    y_true = np.vectorize(map_func)(y_true)\n",
    "    y_pred = np.vectorize(map_func)(y_pred)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('full_dataset/full_dataset.csv')\n",
    "data = df[['emotion', 'preprocessed_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=42)\n",
    "\n",
    "for train_index, test_valid_index in split.split(data, data['emotion']):\n",
    "    train_set = data.iloc[train_index]\n",
    "    test_valid_set = data.iloc[test_valid_index]\n",
    "\n",
    "split2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "for test_index, valid_index in split2.split(test_valid_set, test_valid_set['emotion']):\n",
    "    test_set = test_valid_set.iloc[test_index]\n",
    "    valid_set = test_valid_set.iloc[valid_index]\n",
    "    \n",
    "X_train = train_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1),\n",
    "                       columns=[\"text\"])\n",
    "X_eval = pd.DataFrame(valid_set.apply(generate_prompt, axis=1),\n",
    "                      columns=[\"text\"])\n",
    "                      \n",
    "y_true = test_set.emotion\n",
    "X_test = pd.DataFrame(test_set.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
    "\n",
    "train_data = Dataset.from_pandas(X_train)\n",
    "eval_data = Dataset.from_pandas(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74294971a9e4aa3b626288458ba091f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5f2f1ce5f94f3a8ab5b77d6498dd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                         use_auth_token=True,\n",
    "                                         )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1961/1961 [15:21<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.406\n",
      "Accuracy for label 0: 0.884\n",
      "Accuracy for label 1: 0.371\n",
      "Accuracy for label 2: 0.458\n",
      "Accuracy for label 3: 0.331\n",
      "Accuracy for label 4: 0.238\n",
      "Accuracy for label 5: 0.194\n",
      "Accuracy for label 6: 0.231\n",
      "Accuracy for label 7: 0.302\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.88      0.45       310\n",
      "           1       0.91      0.37      0.53       299\n",
      "           2       0.44      0.46      0.45       310\n",
      "           3       0.77      0.33      0.46       323\n",
      "           4       0.81      0.24      0.37       231\n",
      "           5       0.60      0.19      0.29       237\n",
      "           6       0.48      0.23      0.31       208\n",
      "           7       0.46      0.30      0.37        43\n",
      "           8       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.41      1961\n",
      "   macro avg       0.53      0.33      0.36      1961\n",
      "weighted avg       0.61      0.41      0.42      1961\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[274   0   5   2   0   0   0   1  28]\n",
      " [ 91 111  39  10   7   6   4   5  26]\n",
      " [111   2 142   2   2   6  17   2  26]\n",
      " [132   1  21 107   0   4   8   1  49]\n",
      " [ 91   2  41   7  55   9   5   0  21]\n",
      " [108   3  27   2   3  46  18   4  26]\n",
      " [ 72   2  46   9   0   6  48   2  23]\n",
      " [ 17   1   5   0   1   0   0  13   6]\n",
      " [  0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./resultsLlama\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8, # 4\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3536e1c71f41bca25f53be2f306481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5883 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e198b1e5fce445088f5329c951678e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1962 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7350' max='7350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7350/7350 2:43:29, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.700800</td>\n",
       "      <td>0.584612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.548300</td>\n",
       "      <td>0.577107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.521600</td>\n",
       "      <td>0.577193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>0.588806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.602798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.515400</td>\n",
       "      <td>0.630641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>0.671996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.331200</td>\n",
       "      <td>0.715355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.723609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(\"trained-model-llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1961/1961 [19:09<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.770\n",
      "Accuracy for label 0: 0.894\n",
      "Accuracy for label 1: 0.886\n",
      "Accuracy for label 2: 0.748\n",
      "Accuracy for label 3: 0.827\n",
      "Accuracy for label 4: 0.706\n",
      "Accuracy for label 5: 0.616\n",
      "Accuracy for label 6: 0.572\n",
      "Accuracy for label 7: 0.953\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88       310\n",
      "           1       0.85      0.89      0.87       299\n",
      "           2       0.70      0.75      0.72       310\n",
      "           3       0.82      0.83      0.82       323\n",
      "           4       0.75      0.71      0.73       231\n",
      "           5       0.69      0.62      0.65       237\n",
      "           6       0.57      0.57      0.57       208\n",
      "           7       0.98      0.95      0.96        43\n",
      "\n",
      "    accuracy                           0.77      1961\n",
      "   macro avg       0.78      0.78      0.78      1961\n",
      "weighted avg       0.77      0.77      0.77      1961\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[277   7   4  11   2   4   5   0   0]\n",
      " [  6 265   3   3   7   5  10   0   0]\n",
      " [  8   7 232  13  23   9  17   1   0]\n",
      " [ 18  11  11 267   2   6   8   0   0]\n",
      " [  4   4  31   8 163  12   9   0   0]\n",
      " [  4   4  25   9  10 146  39   0   0]\n",
      " [  2  11  24  13  10  29 119   0   0]\n",
      " [  0   1   0   1   0   0   0  41   0]\n",
      " [  0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame({'text': X_test[\"text\"], \n",
    "                           'y_true':y_true, \n",
    "                           'y_pred': y_pred},\n",
    "                         )\n",
    "evaluation.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
