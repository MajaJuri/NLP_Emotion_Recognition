{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8426b7e-0dd4-4f72-9b7c-f4ae98c5a745",
   "metadata": {},
   "source": [
    "# Traditional ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b6e8d0-3590-4b0a-8cb8-1e3a8df16328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, multilabel_confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3a63eb3-5420-441e-8fe9-fffc82d47805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>['game', 'hurt']</td>\n",
       "      <td>game hurt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;sexuality shouldn’t be a grouping category I...</td>\n",
       "      <td>['sexuality', 'grouping', 'category', 'make', ...</td>\n",
       "      <td>sexuality grouping category make different oth...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>['right', 'dont', 'care', 'fuck']</td>\n",
       "      <td>right dont care fuck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>['man', 'love', 'reddit']</td>\n",
       "      <td>man love reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>['name', 'nowhere', 'near', 'falcon']</td>\n",
       "      <td>name nowhere near falcon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210548</th>\n",
       "      <td>Everyone likes [NAME].</td>\n",
       "      <td>['everyone', 'like', 'name']</td>\n",
       "      <td>everyone like name</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210549</th>\n",
       "      <td>Well when you’ve imported about a gazillion of...</td>\n",
       "      <td>['well', 'imported', 'gazillion', 'country', '...</td>\n",
       "      <td>well imported gazillion country get serious</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210550</th>\n",
       "      <td>That looks amazing</td>\n",
       "      <td>['look', 'amazing']</td>\n",
       "      <td>look amazing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210551</th>\n",
       "      <td>The FDA has plenty to criticize. But like here...</td>\n",
       "      <td>['fda', 'plenty', 'criticize', 'like', 'usuall...</td>\n",
       "      <td>fda plenty criticize like usually criticized h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210552</th>\n",
       "      <td>Desktop link: ^^/r/HelperBot_ ^^Downvote ^^to ...</td>\n",
       "      <td>['desktop', 'link', 'rhelperbot', 'downvote', ...</td>\n",
       "      <td>desktop link rhelperbot downvote remove counte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210553 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original_text  \\\n",
       "0                                         That game hurt.   \n",
       "1        >sexuality shouldn’t be a grouping category I...   \n",
       "2          You do right, if you don't care then fuck 'em!   \n",
       "3                                      Man I love reddit.   \n",
       "4       [NAME] was nowhere near them, he was by the Fa...   \n",
       "...                                                   ...   \n",
       "210548                             Everyone likes [NAME].   \n",
       "210549  Well when you’ve imported about a gazillion of...   \n",
       "210550                                 That looks amazing   \n",
       "210551  The FDA has plenty to criticize. But like here...   \n",
       "210552  Desktop link: ^^/r/HelperBot_ ^^Downvote ^^to ...   \n",
       "\n",
       "                                              text_tokens  \\\n",
       "0                                        ['game', 'hurt']   \n",
       "1       ['sexuality', 'grouping', 'category', 'make', ...   \n",
       "2                       ['right', 'dont', 'care', 'fuck']   \n",
       "3                               ['man', 'love', 'reddit']   \n",
       "4                   ['name', 'nowhere', 'near', 'falcon']   \n",
       "...                                                   ...   \n",
       "210548                       ['everyone', 'like', 'name']   \n",
       "210549  ['well', 'imported', 'gazillion', 'country', '...   \n",
       "210550                                ['look', 'amazing']   \n",
       "210551  ['fda', 'plenty', 'criticize', 'like', 'usuall...   \n",
       "210552  ['desktop', 'link', 'rhelperbot', 'downvote', ...   \n",
       "\n",
       "                                        preprocessed_text  admiration  \\\n",
       "0                                               game hurt           0   \n",
       "1       sexuality grouping category make different oth...           0   \n",
       "2                                    right dont care fuck           0   \n",
       "3                                         man love reddit           0   \n",
       "4                                name nowhere near falcon           0   \n",
       "...                                                   ...         ...   \n",
       "210548                                 everyone like name           0   \n",
       "210549        well imported gazillion country get serious           0   \n",
       "210550                                       look amazing           1   \n",
       "210551  fda plenty criticize like usually criticized h...           0   \n",
       "210552  desktop link rhelperbot downvote remove counte...           0   \n",
       "\n",
       "        amusement  anger  annoyance  approval  caring  confusion  ...  love  \\\n",
       "0               0      0          0         0       0          0  ...     0   \n",
       "1               0      0          0         0       0          0  ...     0   \n",
       "2               0      0          0         0       0          0  ...     0   \n",
       "3               0      0          0         0       0          0  ...     1   \n",
       "4               0      0          0         0       0          0  ...     0   \n",
       "...           ...    ...        ...       ...     ...        ...  ...   ...   \n",
       "210548          0      0          0         0       0          0  ...     1   \n",
       "210549          0      0          0         0       1          0  ...     0   \n",
       "210550          0      0          0         0       0          0  ...     0   \n",
       "210551          0      1          0         0       0          0  ...     0   \n",
       "210552          0      0          0         0       0          0  ...     0   \n",
       "\n",
       "        nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
       "0                 0         0      0            0       0        0        1   \n",
       "1                 0         0      0            0       0        0        0   \n",
       "2                 0         0      0            0       0        0        0   \n",
       "3                 0         0      0            0       0        0        0   \n",
       "4                 0         0      0            0       0        0        0   \n",
       "...             ...       ...    ...          ...     ...      ...      ...   \n",
       "210548            0         0      0            0       0        0        0   \n",
       "210549            0         0      0            0       0        0        0   \n",
       "210550            0         0      0            0       0        0        0   \n",
       "210551            0         0      0            0       0        0        0   \n",
       "210552            0         0      0            0       0        0        0   \n",
       "\n",
       "        surprise  neutral  \n",
       "0              0        0  \n",
       "1              0        0  \n",
       "2              0        1  \n",
       "3              0        0  \n",
       "4              0        1  \n",
       "...          ...      ...  \n",
       "210548         0        0  \n",
       "210549         0        0  \n",
       "210550         0        0  \n",
       "210551         0        0  \n",
       "210552         0        0  \n",
       "\n",
       "[210553 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'full_dataset/full_dataset.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78bd2162-690e-4afb-b9ff-effc5f245263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
      "       'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
      "       'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
      "       'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
      "       'relief', 'remorse', 'sadness', 'surprise', 'neutral'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "class_names = data.drop(columns=['original_text', 'text_tokens', 'preprocessed_text']).columns\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f77b92cf-fc40-459b-93fd-edb4931fd3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size =  168442\n",
      "Test set size =  42111\n"
     ]
    }
   ],
   "source": [
    "X = data['original_text']\n",
    "Y = data.drop(columns=['original_text', 'text_tokens', 'preprocessed_text'], axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=357)\n",
    "\n",
    "print(\"Training set size = \", X_train.shape[0])\n",
    "print(\"Test set size = \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be54401-36aa-4599-8420-65745b51960a",
   "metadata": {},
   "source": [
    "## Klasifikatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7722bb94-514a-4412-9ebd-42aa2588ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    '''\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    '''\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Set size\n",
    "    fig.set_size_inches(12.5, 7.5)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36a69ab7-b72a-4054-89ef-09cf52cb39cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove links\n",
    "    text = re.sub(r\"http\\S*|\\S*\\.com\\S*|\\S*www\\S*\", \" \", text)\n",
    "\n",
    "    # Remove @mentions\n",
    "    text = re.sub(r\"\\s@\\S+\", \" \", text)\n",
    "\n",
    "    # Remove all punctuation\n",
    "    punctuation_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    text = text.translate(punctuation_table)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    tokens = [token for token in tokens if len(token)>2]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a014a12-a409-4dd7-9b9d-d6e18aab9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')  # Adjust parameters as needed\n",
    "\n",
    "X_train_vect = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test_vect = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62042a81-d970-48fd-b2fc-941edd8a7a00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
